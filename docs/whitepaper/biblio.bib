
@inreference{_thread_2017,
	title = {Thread (computing)},
	rights = {Creative Commons Attribution-{ShareAlike} License},
	url = {https://en.wikipedia.org/w/index.php?title=Thread_(computing)&oldid=769356190},
	abstract = {In computer science, a thread of execution is the smallest sequence of programmed instructions that can be managed independently by a scheduler, which is typically a part of the operating system. The implementation of threads and processes differs between operating systems, but in most cases a thread is a component of a process. Multiple threads can exist within one process, executing concurrently and sharing resources such as memory, while different processes do not share these resources. In particular, the threads of a process share its executable code and the values of its variables at any given time.},
	booktitle = {Wikipedia},
	urldate = {2017-03-15},
	date = {2017-03-09},
	langid = {english},
	note = {Page Version {ID}: 769356190},
	file = {Snapshot:/home/prodatalab/.zotero/zotero/7wd6pbl2.default/zotero/storage/8SR8BGT5/index.html:text/html}
}

@inreference{_intermediate_2016,
	title = {Intermediate representation},
	rights = {Creative Commons Attribution-{ShareAlike} License},
	url = {https://en.wikipedia.org/w/index.php?title=Intermediate_representation&oldid=743779005},
	abstract = {An Intermediate representation ({IR}) is the data structure or code used internally by a compiler or virtual machine to represent source code. An {IR} is designed to be conducive for further processing, such as optimization and translation. A "good" {IR} must be accurate – capable of representing the source code without loss of information – and independent of any particular source or target language. An {IR} may take one of several forms: an in-memory data structure, or a special tuple- or stack-based code readable by the program. In the latter case it is also called an intermediate language.
A canonical example is found in most modern compilers, where the linear human-readable text representing a program is transformed into an intermediate graph structure that allows flow analysis and re-arrangement before creating a sequence of actual {CPU} instructions. Use of an intermediate representation such as this allows compiler systems like the {GNU} Compiler Collection and {LLVM} to be used by many different source languages to generate code for many different target architectures.},
	booktitle = {Wikipedia},
	urldate = {2017-03-15},
	date = {2016-10-11},
	langid = {english},
	note = {Page Version {ID}: 743779005},
	file = {Snapshot:/home/prodatalab/.zotero/zotero/7wd6pbl2.default/zotero/storage/38DRG5ZM/index.html:text/html}
}

@inreference{_translator_2017,
	title = {Translator (computing)},
	rights = {Creative Commons Attribution-{ShareAlike} License},
	url = {https://en.wikipedia.org/w/index.php?title=Translator_(computing)&oldid=769938201},
	abstract = {A translator is a computer program that performs the translation of a program written in a given programming language into a functionally equivalent program in a different computer language, without losing the functional or logical structure of the original code (the "essence" of each program). These include translations between high-level and human-readable computer languages such as C++, Java and {COBOL}, intermediate-level languages such as Java bytecode, low-level languages such as the assembly language and machine code, and between similar levels of language on different computing platforms, as well as from any of these to any other of these. Arguably they also include translators between software implementations and hardware/{ASIC} microchip implementations of the same program, and from software descriptions of a microchip to the logic gates needed to build it.
Examples of widely used types of computer languages translators include interpreters, compilers and decompilers, and assemblers and disassemblers.},
	booktitle = {Wikipedia},
	urldate = {2017-03-15},
	date = {2017-03-12},
	langid = {english},
	note = {Page Version {ID}: 769938201},
	file = {Snapshot:/home/prodatalab/.zotero/zotero/7wd6pbl2.default/zotero/storage/A49MHRMJ/index.html:text/html}
}

@online{_boinc_0,
	title = {{BOINC}},
	url = {https://boinc.berkeley.edu/},
	urldate = {2017-03-15},
	date = {0000},
	file = {BOINC:/home/prodatalab/.zotero/zotero/7wd6pbl2.default/zotero/storage/54IUZSI2/boinc.berkeley.edu.html:text/html}
}

@inreference{_stream_2017,
	title = {Stream processing},
	rights = {Creative Commons Attribution-{ShareAlike} License},
	url = {https://en.wikipedia.org/w/index.php?title=Stream_processing&oldid=769493953},
	abstract = {Stream processing is a computer programming paradigm, equivalent to dataflow programming, event stream processing, and reactive programming, that allows some applications to more easily exploit a limited form of parallel processing. Such applications can use multiple computational units, such as the floating point unit on a graphics processing unit or field-programmable gate arrays ({FPGAs}), without explicitly managing allocation, synchronization, or communication among those units.
The stream processing paradigm simplifies parallel software and hardware by restricting the parallel computation that can be performed. Given a sequence of data (a stream), a series of operations (kernel functions) is applied to each element in the stream. Uniform streaming, where one kernel function is applied to all elements in the stream, is typical. Kernel functions are usually pipelined, and local on-chip memory is reused to minimize external memory bandwidth. Since the kernel and stream abstractions expose data dependencies, compiler tools can fully automate and optimize on-chip management tasks. Stream processing hardware can use scoreboarding, for example, to initiate a direct memory access ({DMA}) when dependencies become known. The elimination of manual {DMA} management reduces software complexity, and the elimination of hardware caches reduces the amount of the area not dedicated to computational units such as arithmetic logic units.
During the 1980s stream processing was explored within dataflow programming. An example is the language {SISAL} (Streams and Iteration in a Single Assignment Language).},
	booktitle = {Wikipedia},
	urldate = {2017-03-16},
	date = {2017-03-09},
	langid = {english},
	note = {Page Version {ID}: 769493953},
	file = {Snapshot:/home/prodatalab/.zotero/zotero/7wd6pbl2.default/zotero/storage/NHT7KMWK/index.html:text/html}
}

@inreference{_compiler_2017,
	title = {Compiler},
	rights = {Creative Commons Attribution-{ShareAlike} License},
	url = {https://en.wikipedia.org/w/index.php?title=Compiler&oldid=768504630},
	abstract = {A compiler is a computer program (or a set of programs) that transforms source code written in a programming language (the source language) into another computer language (the target language), with the latter often having a binary form known as object code. The most common reason for converting source code is to create an executable program.
The name "compiler" is primarily used for programs that translate source code from a high-level programming language to a lower level language (e.g., assembly language or machine code). If the compiled program can run on a computer whose {CPU} or operating system is different from the one on which the compiler runs, the compiler is known as a cross-compiler. More generally, compilers are a specific type of translator.
While all programs that take a set of programming specifications and translate them, i.e. create a means to execute those specifications, are technically "compilers", the term generally means a program that produces a separate executable from the compiler (that may require a run time library or subsystem to operate), a compiler that merely executes the original specifications is usually referred to as an "interpreter", although because of differing methods of analyzing what represents compilation and what represents interpretation, there is some overlap between the two terms.
A program that translates from a low-level language to a higher level one is a decompiler. A program that translates between high-level languages is usually called a source-to-source compiler or transpiler. A language rewriter is usually a program that translates the form of expressions without a change of language. The term compiler-compiler is sometimes used to refer to a parser generator, a tool often used to help create the lexer and parser. A compiler is likely to perform many or all of the following operations: lexical analysis, preprocessing, parsing, semantic analysis (syntax-directed translation), code generation, and code optimization. Program faults caused by incorrect compiler behavior can be very difficult to track down and work around; therefore, compiler implementors invest significant effort to ensure compiler correctness.},
	booktitle = {Wikipedia},
	urldate = {2017-03-15},
	date = {2017-03-04},
	langid = {english},
	note = {Page Version {ID}: 768504630},
	file = {Snapshot:/home/prodatalab/.zotero/zotero/7wd6pbl2.default/zotero/storage/QCZBTA3R/index.html:text/html}
}

@inreference{_scoreboarding_2016,
	title = {Scoreboarding},
	rights = {Creative Commons Attribution-{ShareAlike} License},
	url = {https://en.wikipedia.org/w/index.php?title=Scoreboarding&oldid=745075093},
	abstract = {Scoreboarding is a centralized method, used in the {CDC} 6600 computer, for dynamically scheduling a pipeline so that the instructions can execute out of order when there are no conflicts and the hardware is available. In a scoreboard, the data dependencies of every instruction are logged. Instructions are released only when the scoreboard determines that there are no conflicts with previously issued and incomplete instructions. If an instruction is stalled because it is unsafe to continue, the scoreboard monitors the flow of executing instructions until all dependencies have been resolved before the stalled instruction is issued.},
	booktitle = {Wikipedia},
	urldate = {2017-03-16},
	date = {2016-10-19},
	langid = {english},
	note = {Page Version {ID}: 745075093},
	file = {Snapshot:/home/prodatalab/.zotero/zotero/7wd6pbl2.default/zotero/storage/G2USM476/index.html:text/html}
}

@online{_how_2013,
	title = {How An Arcane Coding Method From 1970s Banking Software Could Save The Sanity Of Web Developers Everywhere},
	url = {https://www.fastcompany.com/3016289/how-an-arcane-coding-method-from-1970s-banking-software-could-save-the-sanity-of-web-develop},
	abstract = {Forty years ago, a Canadian bank pioneered a brand new computer system that allowed non-programmers to help write code. The paradigm was so disruptive that it was ignored by computer scientists for decades. But as web apps get increasingly complex, and web devs become increasingly stressed out, “flow-based programming” may be raging back to life.},
	titleaddon = {Fast Company},
	urldate = {2017-03-15},
	date = {2013-08-23},
	file = {Snapshot:/home/prodatalab/.zotero/zotero/7wd6pbl2.default/zotero/storage/I8KJSGSH/how-an-arcane-coding-method-from-1970s-banking-software-could-save-the-sanity-of-web-develop.html:text/html}
}

@inreference{_john_0,
	title = {John Paul Morrison},
	rights = {Creative Commons Attribution-{ShareAlike} License},
	url = {https://en.wikipedia.org/wiki/John_Paul_Morrison},
	booktitle = {Wikipedia},
	urldate = {2017-03-15},
	date = {0000},
	langid = {english},
	file = {Snapshot:/home/prodatalab/.zotero/zotero/7wd6pbl2.default/zotero/storage/N74UQCW2/John_Paul_Morrison.html:text/html}
}

@inreference{_lexical_2017,
	title = {Lexical analysis},
	rights = {Creative Commons Attribution-{ShareAlike} License},
	url = {https://en.wikipedia.org/w/index.php?title=Lexical_analysis&oldid=768879606},
	abstract = {In computer science, lexical analysis is the process of converting a sequence of characters (such as in a computer program or web page) into a sequence of tokens (strings with an assigned and thus identified meaning). A program that performs lexical analysis may be termed a lexer, tokenizer, or scanner, though scanner is also a term for the first stage of a lexer. A lexer is generally combined with a parser, which together analyze the syntax of programming languages, web pages, and so forth.},
	booktitle = {Wikipedia},
	urldate = {2017-03-15},
	date = {2017-03-06},
	langid = {english},
	note = {Page Version {ID}: 768879606},
	file = {Snapshot:/home/prodatalab/.zotero/zotero/7wd6pbl2.default/zotero/storage/XRSCCAU5/index.html:text/html}
}

@inreference{_cryptocurrency_2017,
	title = {Cryptocurrency},
	rights = {Creative Commons Attribution-{ShareAlike} License},
	url = {https://en.wikipedia.org/w/index.php?title=Cryptocurrency&oldid=770074597},
	abstract = {A cryptocurrency (or crypto currency) is a digital asset designed to work as a medium of exchange using cryptography to secure the transactions and to control the creation of additional units of the currency. Cryptocurrencies are a subset of alternative currencies, or specifically of digital currencies.
Bitcoin became the first decentralized cryptocurrency in 2009. Since then, numerous cryptocurrencies have been created. These are frequently called altcoins, as a blend of bitcoin alternative. Bitcoin and its derivatives use decentralized control as opposed to centralized electronic money/centralized banking systems. The decentralized control is related to the use of bitcoin's blockchain transaction database in the role of a distributed ledger.},
	booktitle = {Wikipedia},
	urldate = {2017-03-15},
	date = {2017-03-13},
	langid = {english},
	note = {Page Version {ID}: 770074597},
	file = {Snapshot:/home/prodatalab/.zotero/zotero/7wd6pbl2.default/zotero/storage/RMRBZVJX/index.html:text/html}
}

@inreference{_cache_2017,
	title = {Cache (computing)},
	rights = {Creative Commons Attribution-{ShareAlike} License},
	url = {https://en.wikipedia.org/w/index.php?title=Cache_(computing)&oldid=770496427},
	abstract = {In computing, a cache /ˈkæʃ/ {KASH}, is a hardware or software component that stores data so future requests for that data can be served faster; the data stored in a cache might be the result of an earlier computation, or the duplicate of data stored elsewhere. A cache hit occurs when the requested data can be found in a cache, while a cache miss occurs when it cannot. Cache hits are served by reading data from the cache, which is faster than recomputing a result or reading from a slower data store; thus, the more requests can be served from the cache, the faster the system performs.
To be cost-effective and to enable efficient use of data, caches must be relatively small. Nevertheless, caches have proven themselves in many areas of computing because access patterns in typical computer applications exhibit the locality of reference. Moreover, access patterns exhibit temporal locality if data is requested again that has been recently requested already, while spatial locality refers to requests for data physically stored close to data that has been already requested.},
	booktitle = {Wikipedia},
	urldate = {2017-03-16},
	date = {2017-03-15},
	langid = {english},
	note = {Page Version {ID}: 770496427},
	file = {Snapshot:/home/prodatalab/.zotero/zotero/7wd6pbl2.default/zotero/storage/VFN9KP4M/index.html:text/html}
}

@inreference{_linker_2017,
	title = {Linker (computing)},
	rights = {Creative Commons Attribution-{ShareAlike} License},
	url = {https://en.wikipedia.org/w/index.php?title=Linker_(computing)&oldid=770246411},
	abstract = {In computing, a linker or link editor is a computer program that takes one or more object files generated by a compiler and combines them into a single executable file, library file, or another object file.
A simpler version that writes its output directly to memory is called the loader, though loading is typically considered a separate process.},
	booktitle = {Wikipedia},
	urldate = {2017-03-15},
	date = {2017-03-14},
	langid = {english},
	note = {Page Version {ID}: 770246411},
	file = {Snapshot:/home/prodatalab/.zotero/zotero/7wd6pbl2.default/zotero/storage/PZ2BPK5A/index.html:text/html}
}

@article{gelernter_coordination_1992,
	title = {Coordination Languages and Their Significance},
	volume = {35},
	issn = {0001-0782},
	url = {http://doi.acm.org/10.1145/129630.129635},
	doi = {10.1145/129630.129635},
	pages = {97--107},
	number = {2},
	journaltitle = {Commun. {ACM}},
	author = {Gelernter, David and Carriero, Nicholas},
	urldate = {2017-03-15},
	date = {1992-02},
	keywords = {coordination languages, Linda}
}

@inreference{_bitcoin_2017,
	title = {Bitcoin},
	rights = {Creative Commons Attribution-{ShareAlike} License},
	url = {https://en.wikipedia.org/w/index.php?title=Bitcoin&oldid=770265053},
	abstract = {Bitcoin is a cryptocurrency and a payment system:3 invented by an unidentified programmer, or group of programmers, under the name of Satoshi Nakamoto. Bitcoin was introduced on 31 October 2008 to a cryptography mailing list, and released as open-source software in 2009. The identity of Nakamoto remains unknown, though many have claimed to know it. The system is peer-to-peer, and transactions take place between users directly, without an intermediary. These transactions are verified by network nodes and recorded in a public distributed ledger called the blockchain, which uses bitcoin as its unit of account. Since the system works without a central repository or single administrator, the U.S. Treasury categorizes bitcoin as a decentralized virtual currency. Bitcoin is often called the first cryptocurrency, although prior systems existed, and it is more correctly described as the first decentralized digital currency. Bitcoin is the largest of its kind in terms of total market value.
Bitcoins are created as a reward in a competition in which users offer their computing power to verify and record bitcoin transactions into the blockchain. This activity is referred to as mining and successful miners are rewarded with transaction fees and newly created bitcoins. Besides being obtained by mining, bitcoins can be exchanged for other currencies, products, and services. When sending bitcoins, users can pay an optional transaction fee to the miners. This may expedite the transaction being confirmed.
As of February 2015, over 100,000 merchants and vendors accept bitcoin as payment. Instead of a 2–3\% fee typically imposed by credit card processors, merchants accepting bitcoins often pay fees of 0\% to less than 2\% of the total purchase. Despite the fourfold increase in the number of merchants accepting bitcoin in 2014, the cryptocurrency did not have much momentum in retail transactions. The European Banking Authority and other sources have warned that bitcoin users are not protected by refund rights or chargebacks.
The use of bitcoin by criminals has attracted the attention of financial regulators, legislative bodies, law enforcement, and media. Criminal activities are primarily focused on darknet markets and theft, though officials in countries such as the United States also recognize that bitcoin can provide legitimate financial services.},
	booktitle = {Wikipedia},
	urldate = {2017-03-15},
	date = {2017-03-14},
	langid = {english},
	note = {Page Version {ID}: 770265053},
	file = {Snapshot:/home/prodatalab/.zotero/zotero/7wd6pbl2.default/zotero/storage/8AIQXZVD/index.html:text/html}
}

@online{_flow-based_0,
	title = {Flow-based Programming :: Introduction},
	url = {http://jpaulmorrison.com/fbp/introduction.html},
	urldate = {2017-03-15},
	date = {0000},
	file = {Flow-based Programming \:\: Introduction:/home/prodatalab/.zotero/zotero/7wd6pbl2.default/zotero/storage/NKRCCA8H/introduction.html:text/html}
}

@inreference{_gpss_2016,
	title = {{GPSS}},
	rights = {Creative Commons Attribution-{ShareAlike} License},
	url = {https://en.wikipedia.org/w/index.php?title=GPSS&oldid=735919243},
	abstract = {General Purpose Simulation System ({GPSS}) (originally Gordon's Programmable Simulation System after creator Geoffrey Gordon; the name was changed when it was decided to release it as a product) is a discrete time simulation general-purpose programming language, where a simulation clock advances in discrete steps. A system is modelled as transactions enter the system and are passed from one service (represented by blocs) to another. This is particularly well-suited for problems such as a factory. {GPSS} is less flexible than simulation languages such as Simula and {SIMSCRIPT} {II}.5 but it is easier to use and more popular.},
	booktitle = {Wikipedia},
	urldate = {2017-03-15},
	date = {2016-08-23},
	langid = {english},
	note = {Page Version {ID}: 735919243},
	file = {Snapshot:/home/prodatalab/.zotero/zotero/7wd6pbl2.default/zotero/storage/2QATIJE9/index.html:text/html}
}

@inreference{_decentralized_2016,
	title = {Decentralized computing},
	rights = {Creative Commons Attribution-{ShareAlike} License},
	url = {https://en.wikipedia.org/w/index.php?title=Decentralized_computing&oldid=756380202},
	abstract = {Decentralized computing is the allocation of resources, both hardware and software, to each individual workstation, or office location. In contrast, centralized computing exists when the majority of functions are carried out, or obtained from a remote centralized location. Decentralized computing is a trend in modern-day business environments. This is the opposite of centralized computing, which was prevalent during the early days of computers. A decentralized computer system has many benefits over a conventional centralized network. Desktop computers have advanced so rapidly, that their potential performance far exceeds the requirements of most business applications. This results in most desktop computers remaining idle (in relation to their full potential). A decentralized system can use the potential of these systems to maximize efficiency. However, it is debatable whether these networks increase overall effectiveness.
All computers have to be updated individually with new software, unlike a centralised computer system. Decentralised systems still enable file sharing and all computers can share peripherals such as printers and scanners as well as modems, allowing all the computers in the network to connect to the internet.
A collection of decentralized computers systems are components of a larger computer network, held together by local stations of equal importance and capability. These systems are capable of running independently of each other.},
	booktitle = {Wikipedia},
	urldate = {2017-03-15},
	date = {2016-12-23},
	langid = {english},
	note = {Page Version {ID}: 756380202},
	file = {Snapshot:/home/prodatalab/.zotero/zotero/7wd6pbl2.default/zotero/storage/26H95HFW/index.html:text/html}
}

@inreference{_dataflow_2017,
	title = {Dataflow programming},
	rights = {Creative Commons Attribution-{ShareAlike} License},
	url = {https://en.wikipedia.org/w/index.php?title=Dataflow_programming&oldid=769431422},
	abstract = {In computer programming, dataflow programming is a programming paradigm that models a program as a directed graph of the data flowing between operations, thus implementing dataflow principles and architecture. Dataflow programming languages share some features of functional languages, and were generally developed in order to bring some functional concepts to a language more suitable for numeric processing. Some authors use the term Datastream instead of Dataflow to avoid confusion with Dataflow Computing or Dataflow architecture, based on an indeterministic machine paradigm. Dataflow programming was pioneered by Jack Dennis and his graduate students at {MIT} in the 1960s.},
	booktitle = {Wikipedia},
	urldate = {2017-03-15},
	date = {2017-03-09},
	langid = {english},
	note = {Page Version {ID}: 769431422},
	file = {Snapshot:/home/prodatalab/.zotero/zotero/7wd6pbl2.default/zotero/storage/BD385NAK/index.html:text/html}
}

@inreference{_decentralized_2017,
	title = {Decentralized autonomous organization},
	rights = {Creative Commons Attribution-{ShareAlike} License},
	url = {https://en.wikipedia.org/w/index.php?title=Decentralized_autonomous_organization&oldid=768882559},
	abstract = {A decentralized autonomous organization ({DAO}), sometimes labeled a decentralized autonomous corporation ({DAC}), is an organization that is run through rules encoded as computer programs called smart contracts. A {DAO}'s financial transaction record and program rules are maintained on a blockchain. There are several examples of this business model. The precise legal status of this type of business organization is unclear.
The best-known example was The {DAO}, a {DAO} for venture capital funding, which was launched with \$150 million in crowdfunding in June 2016 and was immediately hacked and drained of {US}\$50 million in cryptocurrency.},
	booktitle = {Wikipedia},
	urldate = {2017-03-15},
	date = {2017-03-06},
	langid = {english},
	note = {Page Version {ID}: 768882559},
	file = {Snapshot:/home/prodatalab/.zotero/zotero/7wd6pbl2.default/zotero/storage/T97ZNEQQ/index.html:text/html}
}

@inreference{_general-purpose_2017,
	title = {General-purpose computing on graphics processing units},
	rights = {Creative Commons Attribution-{ShareAlike} License},
	url = {https://en.wikipedia.org/w/index.php?title=General-purpose_computing_on_graphics_processing_units&oldid=768154348},
	abstract = {General-purpose computing on graphics processing units ({GPGPU}, rarely {GPGP} or {GP}²U) is the use of a graphics processing unit ({GPU}), which typically handles computation only for computer graphics, to perform computation in applications traditionally handled by the central processing unit ({CPU}). The use of multiple video cards in one computer, or large numbers of graphics chips, further parallelizes the already parallel nature of graphics processing. In addition, even a single {GPU}-{CPU} framework provides advantages that multiple {CPUs} on their own do not offer due to the specialization in each chip.
Essentially, a {GPGPU} pipeline is a kind of parallel processing between one or more {GPUs} and {CPUs} that analyzes data as if it were in image or other graphic form. While {GPUs} operate at lower frequencies, they typically have many times the number of cores. Thus, {GPUs} can operate on pictures and graphical data effectively far faster than a traditional {CPU}. Migrating data into graphical form and then using the {GPU} to scan and analyze it can result in profound speedup.
{GPGPU} pipelines were first developed for better, more general graphics processing (e.g., for better shaders). These pipelines were found to fit scientific computing needs well, and have since been developed in this direction.},
	booktitle = {Wikipedia},
	urldate = {2017-03-16},
	date = {2017-03-02},
	langid = {english},
	note = {Page Version {ID}: 768154348},
	file = {Snapshot:/home/prodatalab/.zotero/zotero/7wd6pbl2.default/zotero/storage/RUBSMJ3U/index.html:text/html}
}

@inreference{_abstraction_2016,
	title = {Abstraction principle (computer programming)},
	rights = {Creative Commons Attribution-{ShareAlike} License},
	url = {https://en.wikipedia.org/w/index.php?title=Abstraction_principle_(computer_programming)&oldid=748948417},
	abstract = {In software engineering and programming language theory, the abstraction principle (or the principle of abstraction) is a basic dictum that aims to reduce duplication of information in a program (usually with emphasis on code duplication) whenever practical by making use of abstractions provided by the programming language or software libraries. The principle is sometimes stated as a recommendation to the programmer, but sometimes stated as a requirement of the programming language, assuming it is self-understood why abstractions are desirable to use. The origins of the principle are uncertain; it has been reinvented a number of times, sometimes under a different name, with slight variations.
When read as recommendation to the programmer, the abstraction principle can be generalized as the "don't repeat yourself" principle, which recommends avoiding the duplication of information in general, and also avoiding the duplication of human effort involved in the software development process.},
	booktitle = {Wikipedia},
	urldate = {2017-03-16},
	date = {2016-11-11},
	langid = {english},
	note = {Page Version {ID}: 748948417},
	file = {Snapshot:/home/prodatalab/.zotero/zotero/7wd6pbl2.default/zotero/storage/2IGPX5F4/index.html:text/html}
}

@online{_problem_0,
	title = {problem {AND} programming {AND} language - Google Search},
	url = {https://www.google.com/search?q=problem+AND+program+AND+language&oq=problem+AND+program+AND+language&aqs=chrome..69i57.1151j0j9&sourceid=chrome&ie=UTF-8#q=problem+AND+programming+AND+language&*},
	urldate = {2017-03-15},
	date = {0000},
	file = {problem AND programming AND language - Google Search:/home/prodatalab/.zotero/zotero/7wd6pbl2.default/zotero/storage/BZ3I57P8/search.html:text/html}
}

@inreference{_central_2017,
	title = {Central processing unit},
	rights = {Creative Commons Attribution-{ShareAlike} License},
	url = {https://en.wikipedia.org/w/index.php?title=Central_processing_unit&oldid=769026468},
	abstract = {A central processing unit ({CPU}) is the electronic circuitry within a computer that carries out the instructions of a computer program by performing the basic arithmetic, logical, control and input/output (I/O) operations specified by the instructions. The computer industry has used the term "central processing unit" at least since the early 1960s. Traditionally, the term "{CPU}" refers to a processor, more specifically to its processing unit and control unit ({CU}), distinguishing these core elements of a computer from external components such as main memory and I/O circuitry.
The form, design and implementation of {CPUs} have changed over the course of their history, but their fundamental operation remains almost unchanged. Principal components of a {CPU} include the arithmetic logic unit ({ALU}) that performs arithmetic and logic operations, processor registers that supply operands to the {ALU} and store the results of {ALU} operations, and a control unit that orchestrates the fetching (from memory) and execution of instructions by directing the coordinated operations of the {ALU}, registers and other components.
Most modern {CPUs} are microprocessors, meaning they are contained on a single integrated circuit ({IC}) chip. An {IC} that contains a {CPU} may also contain memory, peripheral interfaces, and other components of a computer; such integrated devices are variously called microcontrollers or systems on a chip ({SoC}). Some computers employ a multi-core processor, which is a single chip containing two or more {CPUs} called "cores"; in that context, one can speak of such single chips as "sockets". Array processors or vector processors have multiple processors that operate in parallel, with no unit considered central. There also exists the concept of virtual {CPUs} which are an abstraction of dynamical aggregated computational resources.},
	booktitle = {Wikipedia},
	urldate = {2017-03-15},
	date = {2017-03-07},
	langid = {english},
	note = {Page Version {ID}: 769026468},
	file = {Snapshot:/home/prodatalab/.zotero/zotero/7wd6pbl2.default/zotero/storage/9N5UTWE7/index.html:text/html}
}

@inreference{_floating-point_2017,
	title = {Floating-point unit},
	rights = {Creative Commons Attribution-{ShareAlike} License},
	url = {https://en.wikipedia.org/w/index.php?title=Floating-point_unit&oldid=770558104},
	abstract = {A floating-point unit ({FPU}, colloquially a math coprocessor) is a part of a computer system specially designed to carry out operations on floating point numbers. Typical operations are addition, subtraction, multiplication, division, square root, and bitshifting. Some systems (particularly older, microcode-based architectures) can also perform various transcendental functions such as exponential or trigonometric calculations, though in most modern processors these are done with software library routines.
In general purpose computer architectures, one or more {FPUs} may be integrated as execution units within the central processing unit; however many embedded processors do not have hardware support for floating-point operations.
When a {CPU} is executing a program that calls for a floating-point operation, there are three ways to carry it out:
A floating-point unit emulator (a floating-point library)
Add-on {FPU}
Integrated {FPU}
Some systems implemented floating point via a coprocessor rather than as an integrated unit. This could be a single integrated circuit, an entire circuit board or a cabinet. Where floating-point calculation hardware has not been provided, floating point calculations are done in software, which takes more processor time but which avoids the cost of the extra hardware. For a particular computer architecture, the floating point unit instructions may be emulated by a library of software functions; this may permit the same object code to run on systems with or without floating point hardware. Emulation can be implemented on any of several levels: in the {CPU} as microcode (not a common practice), as an operating system function, or in user space code. When only integer functionality is available the {CORDIC} floating point emulation methods are most commonly used.
In most modern computer architectures, there is some division of floating-point operations from integer operations. This division varies significantly by architecture; some, like the Intel x86 have dedicated floating-point registers, while some take it as far as independent clocking schemes.
Floating-point operations are often pipelined. In earlier superscalar architectures without general out-of-order execution, floating-point operations were sometimes pipelined separately from integer operations. Since the early 1990s, many microprocessors for desktops and servers have more than one {FPU}.},
	booktitle = {Wikipedia},
	urldate = {2017-03-16},
	date = {2017-03-16},
	langid = {english},
	note = {Page Version {ID}: 770558104},
	file = {Snapshot:/home/prodatalab/.zotero/zotero/7wd6pbl2.default/zotero/storage/AAXE3R4N/index.html:text/html}
}

@inreference{_component-based_2017,
	title = {Component-based software engineering},
	rights = {Creative Commons Attribution-{ShareAlike} License},
	url = {https://en.wikipedia.org/w/index.php?title=Component-based_software_engineering&oldid=766603755},
	abstract = {Component-based software engineering ({CBSE}), also known as component-based development ({CBD}), is a branch of software engineering that emphasizes the separation of concerns with respect to the wide-ranging functionality available throughout a given software system. It is a reuse-based approach to defining, implementing and composing loosely coupled independent components into systems. This practice aims to bring about an equally wide-ranging degree of benefits in both the short-term and the long-term for the software itself and for organizations that sponsor such software.
Software engineering practitioners regard components as part of the starting platform for service-orientation. Components play this role, for example, in web services, and more recently, in service-oriented architectures ({SOA}), whereby a component is converted by the web service into a service and subsequently inherits further characteristics beyond that of an ordinary component.
Components can produce or consume events and can be used for event-driven architectures ({EDA}).},
	booktitle = {Wikipedia},
	urldate = {2017-03-15},
	date = {2017-02-21},
	langid = {english},
	note = {Page Version {ID}: 766603755},
	file = {Snapshot:/home/prodatalab/.zotero/zotero/7wd6pbl2.default/zotero/storage/SDXZ88JS/index.html:text/html}
}

@inreference{_stream_2017-1,
	title = {Stream (computing)},
	rights = {Creative Commons Attribution-{ShareAlike} License},
	url = {https://en.wikipedia.org/w/index.php?title=Stream_(computing)&oldid=767537000},
	abstract = {In computer science, a stream is a sequence of data elements made available over time. A stream can be thought of as items on a conveyor belt being processed one at a time rather than in large batches.
Streams are processed differently from batch data – normal functions cannot operate on streams as a whole, as they have potentially unlimited data, and formally, streams are codata (potentially unlimited), not data (which is finite). Functions that operate on a stream, producing another stream, are known as filters, and can be connected in pipelines, analogously to function composition. Filters may operate on one item of a stream at a time, or may base an item of output on multiple items of input, such as a moving average.},
	booktitle = {Wikipedia},
	urldate = {2017-03-16},
	date = {2017-02-26},
	langid = {english},
	note = {Page Version {ID}: 767537000},
	file = {Snapshot:/home/prodatalab/.zotero/zotero/7wd6pbl2.default/zotero/storage/PRI89M6P/index.html:text/html}
}

@inreference{_parsing_2017,
	title = {Parsing},
	rights = {Creative Commons Attribution-{ShareAlike} License},
	url = {https://en.wikipedia.org/w/index.php?title=Parsing&oldid=769670060},
	abstract = {Parsing ({US} /ˈpɑːrsɪŋ/; {UK} /ˈpɑːrzɪŋ/), syntax analysis or syntactic analysis is the process of analysing a string of symbols, either in natural language or in computer languages, conforming to the rules of a formal grammar. The term parsing comes from Latin pars (orationis), meaning part (of speech).
The term has slightly different meanings in different branches of linguistics and computer science. Traditional sentence parsing is often performed as a method of understanding the exact meaning of a sentence or word, sometimes with the aid of devices such as sentence diagrams. It usually emphasizes the importance of grammatical divisions such as subject and predicate.
Within computational linguistics the term is used to refer to the formal analysis by a computer of a sentence or other string of words into its constituents, resulting in a parse tree showing their syntactic relation to each other, which may also contain semantic and other information.
The term is also used in psycholinguistics when describing language comprehension. In this context, parsing refers to the way that human beings analyze a sentence or phrase (in spoken language or text) "in terms of grammatical constituents, identifying the parts of speech, syntactic relations, etc." This term is especially common when discussing what linguistic cues help speakers to interpret garden-path sentences.
Within computer science, the term is used in the analysis of computer languages, referring to the syntactic analysis of the input code into its component parts in order to facilitate the writing of compilers and interpreters. The term may also be used to describe a split or separation.},
	booktitle = {Wikipedia},
	urldate = {2017-03-15},
	date = {2017-03-10},
	langid = {english},
	note = {Page Version {ID}: 769670060},
	file = {Snapshot:/home/prodatalab/.zotero/zotero/7wd6pbl2.default/zotero/storage/BRMTNNKF/index.html:text/html}
}

@inreference{_source_2017,
	title = {Source code},
	rights = {Creative Commons Attribution-{ShareAlike} License},
	url = {https://en.wikipedia.org/w/index.php?title=Source_code&oldid=770422997},
	abstract = {In computing, source code is any collection of computer instructions, possibly with comments, written using a human-readable programming language, usually as ordinary text. The source code of a program is specially designed to facilitate the work of computer programmers, who specify the actions to be performed by a computer mostly by writing source code. The source code is often transformed by an assembler or compiler into binary machine code understood by the computer. The machine code might then be stored for execution at a later time. Alternatively, source code may be interpreted and thus immediately executed.
Most application software is distributed in a form that includes only executable files. If the source code were included it would be useful to a user, programmer or a system administrator, any of whom might wish to study or modify the program.},
	booktitle = {Wikipedia},
	urldate = {2017-03-15},
	date = {2017-03-15},
	langid = {english},
	note = {Page Version {ID}: 770422997},
	file = {Snapshot:/home/prodatalab/.zotero/zotero/7wd6pbl2.default/zotero/storage/WGERR4ZT/index.html:text/html}
}

@online{_flow-based_0-1,
	title = {Flow-based Programming},
	url = {http://www.jpaulmorrison.com/fbp/},
	urldate = {2017-03-15},
	date = {0000},
	file = {Flow-based Programming:/home/prodatalab/.zotero/zotero/7wd6pbl2.default/zotero/storage/3GKHHW3Q/fbp.html:text/html}
}

@inreference{_melvin_2016,
	title = {Melvin Conway},
	rights = {Creative Commons Attribution-{ShareAlike} License},
	url = {https://en.wikipedia.org/w/index.php?title=Melvin_Conway&oldid=705398278},
	abstract = {Melvin Edward Conway is a computer scientist, computer programmer, and hacker who coined what's now known as Conway's Law: "Organizations which design systems are constrained to produce designs which are copies of the communication structures of these organizations."
Apart from the above, Conway is perhaps most famous for his seminal paper on coroutines. In this paper he proposed to organize compiler as a set of coroutines, which gives possibility of using separate passes in debugging and then running a single pass compiler in production. Another famous paper is his exposition of {UNCOL} in 1958.
Conway wrote an assembler for the Burroughs model 220 computer called {SAVE}. The name {SAVE} was not an acronym, but a feature: programmers lost fewer punched card decks because they all had "{SAVE}" written on them.
His work on Pascal compiler for Rockwell Semiconductor (an immediate-turnaround Pascal trainer for the Rockwell {AIM}-65) led to an arrangement between Apple and Think Technologies (where he served as a principal) under which the latter produced the original (1984) Mac Pascal and Apple {II} Instant Pascal.
In the 1970s and 1980s, he was involved with the {MUMPS} medical computer language and system language standard specification for the National Bureau of Standards. He also wrote a reference book on {MUMPS}.},
	booktitle = {Wikipedia},
	urldate = {2017-03-15},
	date = {2016-02-17},
	langid = {english},
	note = {Page Version {ID}: 705398278},
	file = {Snapshot:/home/prodatalab/.zotero/zotero/7wd6pbl2.default/zotero/storage/QHAPHJQS/index.html:text/html}
}

@inreference{_context_2017,
	title = {Context switch},
	rights = {Creative Commons Attribution-{ShareAlike} License},
	url = {https://en.wikipedia.org/w/index.php?title=Context_switch&oldid=762672473},
	abstract = {In computing, a context switch is the process of storing and restoring the state (more specifically, the execution context) of a process or thread so that execution can be resumed from the same point at a later time. This enables multiple processes to share a single {CPU} and is an essential feature of a multitasking operating system.
The precise meaning of "context switch" varies significantly in usage, most often to mean "thread switch or process switch" or "process switch only", either of which may be referred to as a "task switch". More finely, one can distinguish thread switch (switching between two threads within a given process), process switch (switching between two processes), mode switch (domain crossing: switching between user mode and kernel mode within a given thread), register switch, a stack frame switch, and address space switch (memory map switch: changing virtual memory to physical memory map). The computational cost of context switches varies significantly depending on what precisely it entails, from little more than a subroutine call for light-weight user processes, to very expensive, though typically much less than that of saving or restoring a process image.},
	booktitle = {Wikipedia},
	urldate = {2017-03-15},
	date = {2017-01-30},
	langid = {english},
	note = {Page Version {ID}: 762672473},
	file = {Snapshot:/home/prodatalab/.zotero/zotero/7wd6pbl2.default/zotero/storage/JRQNCJRD/index.html:text/html}
}

@inreference{_open-source_2017,
	title = {Open-source governance},
	rights = {Creative Commons Attribution-{ShareAlike} License},
	url = {https://en.wikipedia.org/w/index.php?title=Open-source_governance&oldid=762629399},
	abstract = {Open-source governance (also known as open politics) is a political philosophy which advocates the application of the philosophies of the open-source and open-content movements to democratic principles to enable any interested citizen to add to the creation of policy, as with a wiki document. Legislation is democratically opened to the general citizenry, employing their collective wisdom to benefit the decision-making process and improve democracy.
Theories on how to constrain, limit or enable this participation vary. Accordingly, there is no one dominant theory of how to go about authoring legislation with this approach. There are a wide array of projects and movements which are working on building open-source governance systems.
Many left-libertarian and radical centrist organizations around the globe have begun advocating open-source governance and its related political ideas as a reformist alternative to current governance systems. Often, these groups have their origins in decentralized structures such as the Internet and place particular importance on the need for anonymity to protect an individual's right to free speech in democratic systems. Opinions vary, however, not least because the principles behind open-source government are still very loosely defined.},
	booktitle = {Wikipedia},
	urldate = {2017-03-15},
	date = {2017-01-29},
	langid = {english},
	note = {Page Version {ID}: 762629399},
	file = {Snapshot:/home/prodatalab/.zotero/zotero/7wd6pbl2.default/zotero/storage/9NN98B4S/index.html:text/html}
}

@inreference{_parallel_2017,
	title = {Parallel computing},
	rights = {Creative Commons Attribution-{ShareAlike} License},
	url = {https://en.wikipedia.org/w/index.php?title=Parallel_computing&oldid=770375706},
	abstract = {Parallel computing is a type of computation in which many calculations or the execution of processes are carried out simultaneously. Large problems can often be divided into smaller ones, which can then be solved at the same time. There are several different forms of parallel computing: bit-level, instruction-level, data, and task parallelism. Parallelism has been employed for many years, mainly in high-performance computing, but interest in it has grown lately due to the physical constraints preventing frequency scaling. As power consumption (and consequently heat generation) by computers has become a concern in recent years, parallel computing has become the dominant paradigm in computer architecture, mainly in the form of multi-core processors.
Parallel computing is closely related to concurrent computing—they are frequently used together, and often conflated, though the two are distinct: it is possible to have parallelism without concurrency (such as bit-level parallelism), and concurrency without parallelism (such as multitasking by time-sharing on a single-core {CPU}). In parallel computing, a computational task is typically broken down in several, often many, very similar subtasks that can be processed independently and whose results are combined afterwards, upon completion. In contrast, in concurrent computing, the various processes often do not address related tasks; when they do, as is typical in distributed computing, the separate tasks may have a varied nature and often require some inter-process communication during execution.
Parallel computers can be roughly classified according to the level at which the hardware supports parallelism, with multi-core and multi-processor computers having multiple processing elements within a single machine, while clusters, {MPPs}, and grids use multiple computers to work on the same task. Specialized parallel computer architectures are sometimes used alongside traditional processors, for accelerating specific tasks.
In some cases parallelism is transparent to the programmer, such as in bit-level or instruction-level parallelism, but explicitly parallel algorithms, particularly those that use concurrency, are more difficult to write than sequential ones, because concurrency introduces several new classes of potential software bugs, of which race conditions are the most common. Communication and synchronization between the different subtasks are typically some of the greatest obstacles to getting good parallel program performance.
A theoretical upper bound on the speed-up of a single program as a result of parallelization is given by Amdahl's law.},
	booktitle = {Wikipedia},
	urldate = {2017-03-16},
	date = {2017-03-15},
	langid = {english},
	note = {Page Version {ID}: 770375706},
	file = {Snapshot:/home/prodatalab/.zotero/zotero/7wd6pbl2.default/zotero/storage/4VKX5FSN/index.html:text/html}
}

@inreference{_direct_2017,
	title = {Direct memory access},
	rights = {Creative Commons Attribution-{ShareAlike} License},
	url = {https://en.wikipedia.org/w/index.php?title=Direct_memory_access&oldid=761715418},
	abstract = {Direct memory access ({DMA}) is a feature of computer systems that allows certain hardware subsystems to access main system memory ({RAM}), independent of the central processing unit ({CPU}).
Without {DMA}, when the {CPU} is using programmed input/output, it is typically fully occupied for the entire duration of the read or write operation, and is thus unavailable to perform other work. With {DMA}, the {CPU} first initiates the transfer, then it does other operations while the transfer is in progress, and it finally receives an interrupt from the {DMA} controller when the operation is done. This feature is useful at any time that the {CPU} cannot keep up with the rate of data transfer, or when the {CPU} needs to perform useful work while waiting for a relatively slow I/O data transfer. Many hardware systems use {DMA}, including disk drive controllers, graphics cards, network cards and sound cards. {DMA} is also used for intra-chip data transfer in multi-core processors. Computers that have {DMA} channels can transfer data to and from devices with much less {CPU} overhead than computers without {DMA} channels. Similarly, a processing element inside a multi-core processor can transfer data to and from its local memory without occupying its processor time, allowing computation and data transfer to proceed in parallel.
{DMA} can also be used for "memory to memory" copying or moving of data within memory. {DMA} can offload expensive memory operations, such as large copies or scatter-gather operations, from the {CPU} to a dedicated {DMA} engine. An implementation example is the I/O Acceleration Technology.},
	booktitle = {Wikipedia},
	urldate = {2017-03-16},
	date = {2017-01-24},
	langid = {english},
	note = {Page Version {ID}: 761715418},
	file = {Snapshot:/home/prodatalab/.zotero/zotero/7wd6pbl2.default/zotero/storage/WWTJ9SPM/index.html:text/html}
}

@inreference{_distributed_2013,
	title = {Distributed data flow},
	rights = {Creative Commons Attribution-{ShareAlike} License},
	url = {https://en.wikipedia.org/w/index.php?title=Distributed_data_flow&oldid=539925767},
	abstract = {Distributed data flow (also abbreviated as distributed flow) refers to a set of events in a distributed application or protocol.
Distributed data flows serve a purpose analogous to variables or method parameters in programming languages such as Java, in that they can represent state that is stored or communicated by a layer of software. Unlike variables or parameters, which represent a unit of state that resides in a single location, distributed flows are dynamic and distributed: they simultaneously appear in multiple locations within the network at the same time. As such, distributed flows are a more natural way of modeling the semantics and inner workings of certain classes of distributed systems. In particular, the distributed data flow abstraction has been used as a convenient way of expressing the high-level logical relationships between parts of distributed protocols.},
	booktitle = {Wikipedia},
	urldate = {2017-03-15},
	date = {2013-02-23},
	langid = {english},
	note = {Page Version {ID}: 539925767},
	file = {Snapshot:/home/prodatalab/.zotero/zotero/7wd6pbl2.default/zotero/storage/ERV7I6VC/index.html:text/html}
}

@inreference{_flow-based_2017,
	title = {Flow-based programming},
	rights = {Creative Commons Attribution-{ShareAlike} License},
	url = {https://en.wikipedia.org/w/index.php?title=Flow-based_programming&oldid=768897923},
	abstract = {In computer programming, flow-based programming ({FBP}) is a programming paradigm that defines applications as networks of "black box" processes, which exchange data across predefined connections by message passing, where the connections are specified externally to the processes. These black box processes can be reconnected endlessly to form different applications without having to be changed internally. {FBP} is thus naturally component-oriented.
{FBP} is a particular form of dataflow programming based on bounded buffers, information packets with defined lifetimes, named ports, and separate definition of connections.},
	booktitle = {Wikipedia},
	urldate = {2017-03-15},
	date = {2017-03-06},
	langid = {english},
	note = {Page Version {ID}: 768897923},
	file = {Snapshot:/home/prodatalab/.zotero/zotero/7wd6pbl2.default/zotero/storage/5SX5BEQX/index.html:text/html}
}

@online{nozomi.hayase_bitcoin_2014,
	title = {Bitcoin, The Beginning Of Open Source Governance?},
	url = {https://falkvinge.net/2014/11/10/bitcoin-the-beginning-of-open-source-governance/},
	abstract = {In my previous article on this site, “Bitcoin, Open Source Movement for Decentralized Future”, among many other issues I addressed some concern about the...},
	titleaddon = {Falkvinge on Liberty},
	author = {{nozomi.hayase}},
	urldate = {2017-03-15},
	date = {2014-11-10},
	file = {Snapshot:/home/prodatalab/.zotero/zotero/7wd6pbl2.default/zotero/storage/7MZXW4QB/bitcoin-the-beginning-of-open-source-governance.html:text/html}
}

@online{sigoure_how_0,
	title = {How long does it take to make a context switch?},
	url = {http://blog.tsunanet.net/2010/11/how-long-does-it-take-to-make-context.html},
	abstract = {That's a interesting question I'm willing to spend some of my time on.  Someone at {StumbleUpon} emitted the hypothesis that with all the impr...},
	author = {Sigoure, Benoit},
	urldate = {2017-03-15},
	date = {0000},
	file = {Snapshot:/home/prodatalab/.zotero/zotero/7wd6pbl2.default/zotero/storage/58ST73JS/how-long-does-it-take-to-make-context.html:text/html}
}

@inreference{_just_in_time_2017,
	title = {Just\_in\_time compilation},
	rights = {Creative Commons Attribution-{ShareAlike} License},
	url = {https://en.wikipedia.org/w/index.php?title=Just-in-time_compilation&oldid=769217130},
	abstract = {In computing, just-in-time ({JIT}) compilation, also known as dynamic translation, is compilation done during execution of a program – at run time – rather than prior to execution. Most often this consists of translation to machine code, which is then executed directly, but can also refer to translation to another format. A system implementing a {JIT} compiler typically continuously analyses the code being executed and identifies parts of the code where the speedup gained from compilation would outweigh the overhead of compiling that code.
{JIT} compilation is a combination of the two traditional approaches to translation to machine code – ahead-of-time compilation ({AOT}), and interpretation – and combines some advantages and drawbacks of both. Roughly, {JIT} compilation combines the speed of compiled code with the flexibility of interpretation, with the overhead of an interpreter and the additional overhead of compiling (not just interpreting). {JIT} compilation is a form of dynamic compilation, and allows adaptive optimization such as dynamic recompilation – thus in theory {JIT} compilation can yield faster execution than static compilation. Interpretation and {JIT} compilation are particularly suited for dynamic programming languages, as the runtime system can handle late-bound data types and enforce security guarantees.},
	booktitle = {Wikipedia},
	urldate = {2017-03-15},
	date = {2017-03-08},
	langid = {english},
	note = {Page Version {ID}: 769217130},
	file = {Snapshot:/home/prodatalab/.zotero/zotero/7wd6pbl2.default/zotero/storage/8CNHHXUF/index.html:text/html}
}

@inreference{_coupling_2017,
	title = {Coupling (computer programming)},
	rights = {Creative Commons Attribution-{ShareAlike} License},
	url = {https://en.wikipedia.org/w/index.php?title=Coupling_(computer_programming)&oldid=767564653},
	abstract = {In software engineering, coupling is the degree of interdependence between software modules; a measure of how closely connected two routines or modules are; the strength of the relationships between modules.
Coupling is usually contrasted with cohesion. Low coupling often correlates with high cohesion, and vice versa. Low coupling is often a sign of a well-structured computer system and a good design, and when combined with high cohesion, supports the general goals of high readability and maintainability.},
	booktitle = {Wikipedia},
	urldate = {2017-03-15},
	date = {2017-02-26},
	langid = {english},
	note = {Page Version {ID}: 767564653},
	file = {Snapshot:/home/prodatalab/.zotero/zotero/7wd6pbl2.default/zotero/storage/THD8VVMT/index.html:text/html}
}

@inreference{_interpreter_2017,
	title = {Interpreter (computing)},
	rights = {Creative Commons Attribution-{ShareAlike} License},
	url = {https://en.wikipedia.org/w/index.php?title=Interpreter_(computing)&oldid=770283029},
	abstract = {In computer science, an interpreter is a computer program that directly executes, i.e. performs, instructions written in a programming or scripting language, without previously compiling them into a machine language program. An interpreter generally uses one of the following strategies for program execution:
parse the source code and perform its behavior directly.
translate source code into some efficient intermediate representation and immediately execute this.
explicitly execute stored precompiled code made by a compiler which is part of the interpreter system.
Early versions of Lisp programming language and Dartmouth {BASIC} would be examples of the first type. Perl, Python, {MATLAB}, and Ruby are examples of the second, while {UCSD} Pascal is an example of the third type. Source programs are compiled ahead of time and stored as machine independent code, which is then linked at run-time and executed by an interpreter and/or compiler (for {JIT} systems). Some systems, such as Smalltalk and contemporary versions of {BASIC} and Java may also combine two and three. Interpreters of various types have also been constructed for many languages traditionally associated with compilation, such as Algol, Fortran, Cobol and C/C++.
While interpretation and compilation are the two main means by which programming languages are implemented, they are not mutually exclusive, as most interpreting systems also perform some translation work, just like compilers. The terms "interpreted language" or "compiled language" signify that the canonical implementation of that language is an interpreter or a compiler, respectively. A high level language is ideally an abstraction independent of particular implementations.},
	booktitle = {Wikipedia},
	urldate = {2017-03-15},
	date = {2017-03-14},
	langid = {english},
	note = {Page Version {ID}: 770283029},
	file = {Snapshot:/home/prodatalab/.zotero/zotero/7wd6pbl2.default/zotero/storage/8EJTKBBK/index.html:text/html}
}

@inreference{_inter-process_2017,
	title = {Inter-process communication},
	rights = {Creative Commons Attribution-{ShareAlike} License},
	url = {https://en.wikipedia.org/w/index.php?title=Inter-process_communication&oldid=768360626},
	abstract = {In computer science, inter-process communication or interprocess communication ({IPC}) refers specifically to the mechanisms an operating system provides to allow processes it manages to share data. Typically, applications can use {IPC}, categorized as clients and servers, where the client requests data and the server responds to client requests. Many applications are both clients and servers, as commonly seen in distributed computing. Methods for achieving {IPC} are divided into categories which vary based on software requirements, such as performance and modularity requirements, and system circumstances, such as network bandwidth and latency.
{IPC} is very important to the design process for microkernels and nanokernels. Microkernels reduce the number of functionalities provided by the kernel. Those functionalities are then obtained by communicating with servers via {IPC}, increasing drastically the number of {IPC} compared to a regular monolithic kernel.},
	booktitle = {Wikipedia},
	urldate = {2017-03-15},
	date = {2017-03-03},
	langid = {english},
	note = {Page Version {ID}: 768360626},
	file = {Snapshot:/home/prodatalab/.zotero/zotero/7wd6pbl2.default/zotero/storage/F36T9R7K/index.html:text/html}
}

@inreference{_optimizing_2017,
	title = {Optimizing compiler},
	rights = {Creative Commons Attribution-{ShareAlike} License},
	url = {https://en.wikipedia.org/w/index.php?title=Optimizing_compiler&oldid=766618235},
	abstract = {In computing, an optimizing compiler is a compiler that tries to minimize or maximize some attributes of an executable computer program. The most common requirement is to minimize the time taken to execute a program; a less common one is to minimize the amount of memory occupied. The growth of portable computers has created a market for minimizing the power consumed by a program. Compiler optimization is generally implemented using a sequence of optimizing transformations, algorithms which take a program and transform it to produce a semantically equivalent output program that uses fewer resources.
It has been shown that some code optimization problems are {NP}-complete, or even undecidable. In practice, factors such as the programmer's willingness to wait for the compiler to complete its task place upper limits on the optimizations that a compiler implementor might provide. (Optimization is generally a very {CPU}- and memory-intensive process.) In the past, computer memory limitations were also a major factor in limiting which optimizations could be performed. Because of all these factors, optimization rarely produces "optimal" output in any sense, and in fact an "optimization" may impede performance in some cases; rather, they are heuristic methods for improving resource usage in typical programs.},
	booktitle = {Wikipedia},
	urldate = {2017-03-15},
	date = {2017-02-21},
	langid = {english},
	note = {Page Version {ID}: 766618235},
	file = {Snapshot:/home/prodatalab/.zotero/zotero/7wd6pbl2.default/zotero/storage/Q36NTCJT/index.html:text/html}
}

@inreference{_data_2017,
	title = {Data dependency},
	rights = {Creative Commons Attribution-{ShareAlike} License},
	url = {https://en.wikipedia.org/w/index.php?title=Data_dependency&oldid=766284477},
	abstract = {A data dependency in computer science is a situation in which a program statement (instruction) refers to the data of a preceding statement. In compiler theory, the technique used to discover data dependencies among statements (or instructions) is called dependence analysis.
There are three types of dependencies: data, name, and control.},
	booktitle = {Wikipedia},
	urldate = {2017-03-16},
	date = {2017-02-19},
	langid = {english},
	note = {Page Version {ID}: 766284477},
	file = {Snapshot:/home/prodatalab/.zotero/zotero/7wd6pbl2.default/zotero/storage/CQBPH2VX/index.html:text/html}
}

@online{pontin_problem_0,
	title = {The Problem with Programming},
	url = {https://www.technologyreview.com/s/406923/the-problem-with-programming/},
	abstract = {Bjarne Stroustrup, the inventor of the C++ programming language, defends his legacy and examines what’s wrong with most software code.},
	titleaddon = {{MIT} Technology Review},
	author = {Pontin, Jason},
	urldate = {2017-03-15},
	date = {0000},
	file = {Snapshot:/home/prodatalab/.zotero/zotero/7wd6pbl2.default/zotero/storage/R36KB9VB/the-problem-with-programming.html:text/html}
}

@inreference{_coroutine_2017,
	title = {Coroutine},
	rights = {Creative Commons Attribution-{ShareAlike} License},
	url = {https://en.wikipedia.org/w/index.php?title=Coroutine&oldid=769155808},
	abstract = {Coroutines are computer program components that generalize subroutines for non-preemptive multitasking, by allowing multiple entry points for suspending and resuming execution at certain locations. Coroutines are well-suited for implementing more familiar program components such as cooperative tasks, exceptions, event loop, iterators, infinite lists and pipes.
According to Donald Knuth, the term coroutine was coined by Melvin Conway in 1958, after he applied it to construction of an assembly program. The first published explanation of the coroutine appeared later, in 1963.},
	booktitle = {Wikipedia},
	urldate = {2017-03-15},
	date = {2017-03-07},
	langid = {english},
	note = {Page Version {ID}: 769155808},
	file = {Snapshot:/home/prodatalab/.zotero/zotero/7wd6pbl2.default/zotero/storage/J2ACDXU6/index.html:text/html}
}

@inreference{_virtualization_2017,
	title = {Virtualization},
	rights = {Creative Commons Attribution-{ShareAlike} License},
	url = {https://en.wikipedia.org/w/index.php?title=Virtualization&oldid=769822930},
	abstract = {In computing, virtualization refers to the act of creating a virtual (rather than actual) version of something, including virtual computer hardware platforms, storage devices, and computer network resources.
Virtualization began in the 1960s, as a method of logically dividing the system resources provided by mainframe computers between different applications. Since then, the meaning of the term has broadened.},
	booktitle = {Wikipedia},
	urldate = {2017-03-15},
	date = {2017-03-11},
	langid = {english},
	note = {Page Version {ID}: 769822930},
	file = {Snapshot:/home/prodatalab/.zotero/zotero/7wd6pbl2.default/zotero/storage/CC3UATVX/index.html:text/html}
}

@inreference{_process_2016,
	title = {Process (computing)},
	rights = {Creative Commons Attribution-{ShareAlike} License},
	url = {https://en.wikipedia.org/w/index.php?title=Process_(computing)&oldid=737223447},
	abstract = {In computing, a process is an instance of a computer program that is being executed. It contains the program code and its current activity. Depending on the operating system ({OS}), a process may be made up of multiple threads of execution that execute instructions concurrently.
A computer program is a passive collection of instructions, while a process is the actual execution of those instructions. Several processes may be associated with the same program; for example, opening up several instances of the same program often means more than one process is being executed.
Multitasking is a method to allow multiple processes to share processors ({CPUs}) and other system resources. Each {CPU} executes a single task at a time. However, multitasking allows each processor to switch between tasks that are being executed without having to wait for each task to finish. Depending on the operating system implementation, switches could be performed when tasks perform input/output operations, when a task indicates that it can be switched, or on hardware interrupts.
A common form of multitasking is time-sharing. Time-sharing is a method to allow fast response for interactive user applications. In time-sharing systems, context switches are performed rapidly, which makes it seem like multiple processes are being executed simultaneously on the same processor. This seeming execution of multiple processes simultaneously is called concurrency.
For security and reliability, most modern operating systems prevent direct communication between independent processes, providing strictly mediated and controlled inter-process communication functionality.},
	booktitle = {Wikipedia},
	urldate = {2017-03-15},
	date = {2016-09-01},
	langid = {english},
	note = {Page Version {ID}: 737223447},
	file = {Snapshot:/home/prodatalab/.zotero/zotero/7wd6pbl2.default/zotero/storage/8MGJCQQV/index.html:text/html}
}

@inreference{_bittorrent_2017,
	title = {{BitTorrent}},
	rights = {Creative Commons Attribution-{ShareAlike} License},
	url = {https://en.wikipedia.org/w/index.php?title=BitTorrent&oldid=768543182},
	abstract = {{BitTorrent} is a communications protocol of peer-to-peer file sharing ("P2P") which is used to distribute data and electronic files over the Internet. {BitTorrent} is one of the most common protocols for transferring large files, such as digital video files containing {TV} shows or video clips or digital audio files containing songs. Peer-to-peer networks have been estimated to collectively account for approximately 43\% to 70\% of all Internet traffic (depending on location) as of February 2009. In November 2004, {BitTorrent} was responsible for 25\% of all Internet traffic. As of February 2013, {BitTorrent} was responsible for 3.35\% of all worldwide bandwidth, more than half of the 6\% of total bandwidth dedicated to file sharing.
To send or receive files, a person uses a {BitTorrent} client on their Internet-connected computer. A {BitTorrent} client is a computer program that implements the {BitTorrent} protocol. Popular clients include μTorrent, Xunlei, Transmission, {qBittorrent}, Vuze, Deluge, {BitComet} and Tixati. {BitTorrent} trackers provide a list of files available for transfer, and allow the client to find peer users known as seeds who may transfer the files.
Programmer Bram Cohen, a former University at Buffalo student, designed the protocol in April 2001 and released the first available version on 2 July 2001, and the most recent version in 2013. {BitTorrent} clients are available for a variety of computing platforms and operating systems including an official client released by {BitTorrent}, Inc.
As of 2013, {BitTorrent} has 15–27 million concurrent users at any time. As of January 2012, {BitTorrent} is utilized by 150 million active users. Based on this figure, the total number of monthly {BitTorrent} users may be estimated to more than a quarter of a billion.},
	booktitle = {Wikipedia},
	urldate = {2017-03-15},
	date = {2017-03-04},
	langid = {english},
	note = {Page Version {ID}: 768543182},
	file = {Snapshot:/home/prodatalab/.zotero/zotero/7wd6pbl2.default/zotero/storage/K38CQEDX/index.html:text/html}
}

@inreference{_multi-core_2017,
	title = {Multi-core processor},
	rights = {Creative Commons Attribution-{ShareAlike} License},
	url = {https://en.wikipedia.org/w/index.php?title=Multi-core_processor&oldid=769864655},
	abstract = {A multi-core processor is a single computing component with two or more independent actual processing units (called "cores"), which are units that read and execute program instructions. The instructions are ordinary {CPU} instructions (such as add, move data, and branch), but the multiple cores can run multiple instructions at the same time, increasing overall speed for programs amenable to parallel computing. Manufacturers typically integrate the cores onto a single integrated circuit die (known as a chip multiprocessor or {CMP}), or onto multiple dies in a single chip package.
A multi-core processor implements multiprocessing in a single physical package. Designers may couple cores in a multi-core device tightly or loosely. For example, cores may or may not share caches, and they may implement message passing or shared-memory inter-core communication methods. Common network topologies to interconnect cores include bus, ring, two-dimensional mesh, and crossbar. Homogeneous multi-core systems include only identical cores; heterogeneous multi-core systems have cores that are not identical (e.g. big.{LITTLE} have heterogeneous cores that share the same instruction set, while {AMD} Accelerated Processing Units have cores that don't even share the same instruction set). Just as with single-processor systems, cores in multi-core systems may implement architectures such as {VLIW}, superscalar, vector, or multithreading.
Multi-core processors are widely used across many application domains, including general-purpose, embedded, network, digital signal processing ({DSP}), and graphics ({GPU}).
The improvement in performance gained by the use of a multi-core processor depends very much on the software algorithms used and their implementation. In particular, possible gains are limited by the fraction of the software that can run in parallel simultaneously on multiple cores; this effect is described by Amdahl's law. In the best case, so-called embarrassingly parallel problems may realize speedup factors near the number of cores, or even more if the problem is split up enough to fit within each core's cache(s), avoiding use of much slower main-system memory. Most applications, however, are not accelerated so much unless programmers invest a prohibitive amount of effort in re-factoring the whole problem. The parallelization of software is a significant ongoing topic of research.},
	booktitle = {Wikipedia},
	urldate = {2017-03-15},
	date = {2017-03-12},
	langid = {english},
	note = {Page Version {ID}: 769864655},
	file = {Snapshot:/home/prodatalab/.zotero/zotero/7wd6pbl2.default/zotero/storage/DE8VTJBG/index.html:text/html}
}

@inreference{_visual_2017,
	title = {Visual programming language},
	rights = {Creative Commons Attribution-{ShareAlike} License},
	url = {https://en.wikipedia.org/w/index.php?title=Visual_programming_language&oldid=768871289},
	abstract = {In computing, a visual programming language ({VPL}) is any programming language that lets users create programs by manipulating program elements graphically rather than by specifying them textually. A {VPL} allows programming with visual expressions, spatial arrangements of text and graphic symbols, used either as elements of syntax or secondary notation. For example, many {VPLs} (known as dataflow or diagrammatic programming) are based on the idea of "boxes and arrows", where boxes or other screen objects are treated as entities, connected by arrows, lines or arcs which represent relations.},
	booktitle = {Wikipedia},
	urldate = {2017-03-15},
	date = {2017-03-06},
	langid = {english},
	note = {Page Version {ID}: 768871289},
	file = {Snapshot:/home/prodatalab/.zotero/zotero/7wd6pbl2.default/zotero/storage/A8MZMF3G/index.html:text/html}
}

@inreference{_blockchain_2017,
	title = {Blockchain},
	rights = {Creative Commons Attribution-{ShareAlike} License},
	url = {https://en.wikipedia.org/w/index.php?title=Blockchain&oldid=770257357},
	abstract = {A blockchain – originally block chain – is a distributed database that maintains a continuously growing list of ordered records called blocks. Each block contains a timestamp and a link to a previous block. By design, blockchains are inherently resistant to modification of the data — once recorded, the data in a block cannot be altered retroactively. Through the use of a peer-to-peer network and a distributed timestamping server, a blockchain database is managed autonomously. Blockchains are "an open, distributed ledger that can record transactions between two parties efficiently and in a verifiable and permanent way. The ledger itself can also be programmed to trigger transactions automatically."
Blockchains are secure by design and an example of a distributed computing system with high byzantine fault tolerance. Decentralised consensus can therefore be achieved with a blockchain. This makes blockchains suitable for the recording of events, medical records, and other records management activities, identity management, transaction processing, and proving data provenance.
The first blockchain was conceptualised by Satoshi Nakamoto in 2008 and implemented the following year as a core component of the digital currency bitcoin, where it serves as the public ledger for all transactions. The invention of the blockchain for bitcoin made it the first digital currency to solve the double spending problem, without the use of a trusted authority or central server. The bitcoin design has been the inspiration for other applications.},
	booktitle = {Wikipedia},
	urldate = {2017-03-15},
	date = {2017-03-14},
	langid = {english},
	note = {Page Version {ID}: 770257357},
	file = {Snapshot:/home/prodatalab/.zotero/zotero/7wd6pbl2.default/zotero/storage/26Q6K88T/index.html:text/html}
}

@inreference{_glossary_2017,
	title = {Glossary of computer hardware terms},
	rights = {Creative Commons Attribution-{ShareAlike} License},
	url = {https://en.wikipedia.org/w/index.php?title=Glossary_of_computer_hardware_terms&oldid=769120683},
	abstract = {This is a glossary of terms relating to computer hardware – physical computer hardware, architectural issues, and peripherals.},
	booktitle = {Wikipedia},
	urldate = {2017-03-16},
	date = {2017-03-07},
	langid = {english},
	note = {Page Version {ID}: 769120683},
	file = {Snapshot:/home/prodatalab/.zotero/zotero/7wd6pbl2.default/zotero/storage/V36DJ8U8/index.html:text/html}
}

@inreference{_raftlib_2017,
	title = {{RaftLib}},
	rights = {Creative Commons Attribution-{ShareAlike} License},
	url = {https://en.wikipedia.org/w/index.php?title=RaftLib&oldid=759225468},
	abstract = {{RaftLib} is a portable parallel processing system that aims to provide extreme performance while increasing programmer productivity. It enables a programmer to assemble a massively parallel program (both local and distributed) using simple iostream-like operators. {RaftLib} handles threading, memory allocation, memory placement, and auto-parallelization of compute kernels. It enables applications to be constructed from chains of compute kernels forming a task and pipeline parallel compute graph. Programs are authored in C++ (although other language bindings are planned).},
	booktitle = {Wikipedia},
	urldate = {2017-03-15},
	date = {2017-01-09},
	langid = {english},
	note = {Page Version {ID}: 759225468},
	file = {Snapshot:/home/prodatalab/.zotero/zotero/7wd6pbl2.default/zotero/storage/XS5UEMBZ/index.html:text/html}
}

@inreference{_microservices_2017,
	title = {Microservices},
	rights = {Creative Commons Attribution-{ShareAlike} License},
	url = {https://en.wikipedia.org/w/index.php?title=Microservices&oldid=765866501},
	abstract = {Microservices is a specialisation of an implementation approach for service-oriented architectures ({SOA}) used to build flexible, independently deployable software systems. Services in a microservice architecture ({MSA}) are processes that communicate with each other over a network in order to fulfill a goal. These services use technology-agnostic protocols. The microservices approach is a first realisation of {SOA} that followed the introduction of {DevOps} and is becoming more popular for building continuously deployed systems.
In a microservices architecture, services should have a small granularity and the protocols should be lightweight. A central microservices property that appears in multiple definitions is that services should be independently deployable. The benefit of distributing different responsibilities of the system into different smaller services is that it enhances the cohesion and decreases the coupling. This makes it easier to change and add functions and qualities to the system at any time. It also allows the architecture of an individual service to emerge through continuous refactoring, and hence reduces the need for a big up-front design and allows for releasing software early and continuously.},
	booktitle = {Wikipedia},
	urldate = {2017-03-15},
	date = {2017-02-16},
	langid = {english},
	note = {Page Version {ID}: 765866501},
	file = {Snapshot:/home/prodatalab/.zotero/zotero/7wd6pbl2.default/zotero/storage/EXXC22NZ/index.html:text/html}
}

@inreference{_compute_2017,
	title = {Compute kernel},
	rights = {Creative Commons Attribution-{ShareAlike} License},
	url = {https://en.wikipedia.org/w/index.php?title=Compute_kernel&oldid=766499954},
	abstract = {In computing, a compute kernel is a routine compiled for high throughput accelerators (such as {GPUs}), {DSPs} or {FPGAs}, separate from (but used by) a main program. They are sometimes called compute shaders, sharing execution units with vertex shaders and pixel shaders on {GPUs}, but are not limited to execution on one class of device, or graphics {APIs}.},
	booktitle = {Wikipedia},
	urldate = {2017-03-16},
	date = {2017-02-20},
	langid = {english},
	note = {Page Version {ID}: 766499954},
	file = {Snapshot:/home/prodatalab/.zotero/zotero/7wd6pbl2.default/zotero/storage/7BBXISPW/index.html:text/html}
}

@inreference{_linda_2017,
	title = {Linda (coordination language)},
	rights = {Creative Commons Attribution-{ShareAlike} License},
	url = {https://en.wikipedia.org/w/index.php?title=Linda_(coordination_language)&oldid=766267772},
	abstract = {In computer science, Linda is a model of coordination and communication among several parallel processes operating upon objects stored in and retrieved from shared, virtual, associative memory. It was developed by Sudhir Ahuja at {AT}\&T Bell Laboratories in collaboration with David Gelernter and Nicholas Carriero at Yale University in 1986.},
	booktitle = {Wikipedia},
	urldate = {2017-03-15},
	date = {2017-02-19},
	langid = {english},
	note = {Page Version {ID}: 766267772},
	file = {Snapshot:/home/prodatalab/.zotero/zotero/7wd6pbl2.default/zotero/storage/9ZGBZDGA/index.html:text/html}
}

@inreference{_latency_2017,
	title = {Latency (engineering)},
	rights = {Creative Commons Attribution-{ShareAlike} License},
	url = {https://en.wikipedia.org/w/index.php?title=Latency_(engineering)&oldid=764966617},
	abstract = {Latency is a time interval between the stimulation and response, or, from a more general point of view, a time delay between the cause and the effect of some physical change in the system being observed. Latency is physically a consequence of the limited velocity with which any physical interaction can propagate. This velocity is always lower than or equal to the speed of light. Therefore, every physical system that has spatial dimensions different from zero will experience some sort of latency, regardless of the nature of stimulation that it has been exposed to.
The precise definition of latency depends on the system being observed and the nature of stimulation. In communications, the lower limit of latency is determined by the medium being used for communications. In reliable two-way communication systems, latency limits the maximum rate that information can be transmitted, as there is often a limit on the amount of information that is "in-flight" at any one moment. In the field of human–machine interaction, perceptible latency has a strong effect on user satisfaction and usability.},
	booktitle = {Wikipedia},
	urldate = {2017-03-15},
	date = {2017-02-11},
	langid = {english},
	note = {Page Version {ID}: 764966617},
	file = {Snapshot:/home/prodatalab/.zotero/zotero/7wd6pbl2.default/zotero/storage/XKS23XKS/index.html:text/html}
}

@inreference{_domain-specific_2017,
	title = {Domain-specific language},
	rights = {Creative Commons Attribution-{ShareAlike} License},
	url = {https://en.wikipedia.org/w/index.php?title=Domain-specific_language&oldid=770043904},
	abstract = {A domain-specific language ({DSL}) is a computer language specialized to a particular application domain. This is in contrast to a general-purpose language ({GPL}), which is broadly applicable across domains. There is a wide variety of {DSLs}, ranging from widely used languages for common domains, such as {HTML} for web pages, down to languages used by only one or a few pieces of software, such as Emacs Lisp for {GNU} Emacs and {XEmacs}. {DSLs} can be further subdivided by the kind of language, and include domain-specific markup languages, domain-specific modeling languages (more generally, specification languages), and domain-specific programming languages. Special-purpose computer languages have always existed in the computer age, but the term "domain-specific language" has become more popular due to the rise of domain-specific modeling. Simpler {DSLs}, particularly ones used by a single application, are sometimes informally called mini-languages.
The line between general-purpose languages and domain-specific languages is not always sharp, as a language may have specialized features for a particular domain but be applicable more broadly, or conversely may in principle be capable of broad application but in practice used primarily for a specific domain. For example, Perl was originally developed as a text-processing and glue language, for the same domain as {AWK} and shell scripts, but was mostly used as a general-purpose programming language later on. By contrast, {PostScript} is a Turing complete language, and in principle can be used for any task, but in practice is narrowly used as a page description language.},
	booktitle = {Wikipedia},
	urldate = {2017-03-15},
	date = {2017-03-13},
	langid = {english},
	note = {Page Version {ID}: 770043904},
	file = {Snapshot:/home/prodatalab/.zotero/zotero/7wd6pbl2.default/zotero/storage/PWDEJDQI/index.html:text/html}
}

@inreference{_semantic_2016,
	title = {Semantic analysis (compilers)},
	rights = {Creative Commons Attribution-{ShareAlike} License},
	url = {https://en.wikipedia.org/w/index.php?title=Semantic_analysis_(compilers)&oldid=740911436},
	abstract = {Semantic analysis, also context sensitive analysis, is a process in compiler construction, usually after parsing, to gather necessary semantic information from the source code. It usually includes type checking, or makes sure a variable is declared before use which is impossible to describe in Extended Backus–Naur Form and thus not easily detected during parsing.},
	booktitle = {Wikipedia},
	urldate = {2017-03-15},
	date = {2016-09-24},
	langid = {english},
	note = {Page Version {ID}: 740911436},
	file = {Snapshot:/home/prodatalab/.zotero/zotero/7wd6pbl2.default/zotero/storage/IVIC9JDV/index.html:text/html}
}

@inreference{_cooperative_2017,
	title = {Cooperative},
	rights = {Creative Commons Attribution-{ShareAlike} License},
	url = {https://en.wikipedia.org/w/index.php?title=Cooperative&oldid=768655639},
	abstract = {A cooperative (also known as co-operative, co-op, or coop) is an autonomous association of people united voluntarily to meet their common economic, social and cultural needs and aspirations through a jointly owned and democratically controlled business. Cooperatives include non-profit community organizations and businesses that are owned and managed by the people who use their services (a consumer cooperative); by the people who work there (a worker cooperative); by the people who live there (a housing cooperative); hybrids such as worker cooperatives that are also consumer cooperatives or credit unions; multi-stakeholder cooperatives such as those that bring together civil society and local actors to deliver community needs; and second and third tier cooperatives whose members are other cooperatives. It was estimated that in 2012 approximately one billion people were members of at least one cooperative and that the turnover of the largest three hundred cooperatives in the world reached \$2.2 trillion – which, if they were to be a country, it would make them the seventh largest.
In short, a co-op can be defined as "a jointly owned enterprise engaging in the production or distribution of goods or the supplying of services, operated by its members for their mutual benefit, typically organized by consumers or farmers." Cooperative businesses are typically more economically resilient than many other forms of enterprise, with twice the number of co-operatives (80\%) surviving their first five years compared with other business ownership models (41\%). Cooperatives frequently have social goals which they aim to accomplish by investing a proportion of trading profits back into their communities. As an example of this, in 2013, retail co-operatives in the {UK} invested 6.9\% of their pre-tax profits in the communities in which they trade as compared with 2.4\% for other rival supermarkets.
The International Co-operative Alliance was the first international association formed by the cooperative movement. It includes the World Council of Credit Unions. A second organization was formed later in Germany, the International Raiffeisen Union. In the United States, the National Cooperative Business Association ({NCBA}) serves as the sector's oldest national membership association. It is dedicated to ensuring that cooperative businesses have the same opportunities as other businesses operating in the country and that consumers have access to cooperatives in the marketplace. A U.S. National Cooperative Bank was formed in the 1970s. By 2004, a new association focused on worker co-ops was founded, the United States Federation of Worker Cooperatives.
Since 2002 cooperatives and credit unions could be distinguished on the Internet by use of a .coop domain. Since 2014, following International Cooperative Alliance's introduction of the Cooperative Marque, {ICA} cooperatives and {WOCCU} credit unions can also be identified by a coop ethical consumerism label.},
	booktitle = {Wikipedia},
	urldate = {2017-03-15},
	date = {2017-03-05},
	langid = {english},
	note = {Page Version {ID}: 768655639},
	file = {Snapshot:/home/prodatalab/.zotero/zotero/7wd6pbl2.default/zotero/storage/GDZW5PD9/index.html:text/html}
}

@inreference{_pipeline_2017,
	title = {Pipeline (computing)},
	rights = {Creative Commons Attribution-{ShareAlike} License},
	url = {https://en.wikipedia.org/w/index.php?title=Pipeline_(computing)&oldid=769305772},
	abstract = {In computing, a pipeline is a set of data processing elements connected in series, where the output of one element is the input of the next one. The elements of a pipeline are often executed in parallel or in time-sliced fashion; in that case, some amount of buffer storage is often inserted between elements.
Computer-related pipelines include:
Instruction pipelines, such as the classic {RISC} pipeline, which are used in central processing units ({CPUs}) to allow overlapping execution of multiple instructions with the same circuitry. The circuitry is usually divided up into stages, including instruction decoding, arithmetic, and register fetching stages, wherein each stage processes one instruction at a time.
Graphics pipelines, found in most graphics processing units ({GPUs}), which consist of multiple arithmetic units, or complete {CPUs}, that implement the various stages of common rendering operations (perspective projection, window clipping, color and light calculation, rendering, etc.).
Software pipelines, where commands can be written where the output of one operation is automatically fed to the next, following operation. The Unix system call pipe is a classic example of this concept, although other operating systems do support pipes as well.
{HTTP} pipelining, where multiple requests are sent without waiting for the result of the first request.},
	booktitle = {Wikipedia},
	urldate = {2017-03-16},
	date = {2017-03-08},
	langid = {english},
	note = {Page Version {ID}: 769305772},
	file = {Snapshot:/home/prodatalab/.zotero/zotero/7wd6pbl2.default/zotero/storage/UT936U5U/index.html:text/html}
}

@inreference{_machine_2017,
	title = {Machine code},
	rights = {Creative Commons Attribution-{ShareAlike} License},
	url = {https://en.wikipedia.org/w/index.php?title=Machine_code&oldid=770243476},
	abstract = {Machine code or machine language is a set of instructions executed directly by a computer's central processing unit ({CPU}). Each instruction performs a very specific task, such as a load, a jump, or an {ALU} operation on a unit of data in a {CPU} register or memory. Every program directly executed by a {CPU} is made up of a series of such instructions.
Machine code may be regarded as the lowest-level representation of a compiled or assembled computer program or as a primitive and hardware-dependent programming language. While it is possible to write programs directly in machine code, it is tedious and error prone to manage individual bits and calculate numerical addresses and constants manually. For this reason machine code is almost never used to write programs.
Almost all practical programs today are written in higher-level languages or assembly language. The source code is then translated to executable machine code by utilities such as interpreters, compilers, assemblers, and/or linkers.},
	booktitle = {Wikipedia},
	urldate = {2017-03-15},
	date = {2017-03-14},
	langid = {english},
	note = {Page Version {ID}: 770243476},
	file = {Snapshot:/home/prodatalab/.zotero/zotero/7wd6pbl2.default/zotero/storage/6TXG8VAF/index.html:text/html}
}

@inreference{_loose_2017,
	title = {Loose coupling},
	rights = {Creative Commons Attribution-{ShareAlike} License},
	url = {https://en.wikipedia.org/w/index.php?title=Loose_coupling&oldid=770324383},
	abstract = {In computing and systems design a loosely coupled system is one in which each of its components has, or makes use of, little or no knowledge of the definitions of other separate components. Subareas include the coupling of classes, interfaces, data, and services. Loose coupling is the opposite of tight coupling.},
	booktitle = {Wikipedia},
	urldate = {2017-03-15},
	date = {2017-03-14},
	langid = {english},
	note = {Page Version {ID}: 770324383},
	file = {Snapshot:/home/prodatalab/.zotero/zotero/7wd6pbl2.default/zotero/storage/XTSTA7IH/index.html:text/html}
}

@inreference{_arithmetic_2017,
	title = {Arithmetic logic unit},
	rights = {Creative Commons Attribution-{ShareAlike} License},
	url = {https://en.wikipedia.org/w/index.php?title=Arithmetic_logic_unit&oldid=770066907},
	abstract = {An arithmetic logic unit ({ALU}) is a combinational digital electronic circuit that performs arithmetic and bitwise operations on integer binary numbers. This is in contrast to a floating-point unit ({FPU}), which operates on floating point numbers. An {ALU} is a fundamental building block of many types of computing circuits, including the central processing unit ({CPU}) of computers, {FPUs}, and graphics processing units ({GPUs}). A single {CPU}, {FPU} or {GPU} may contain multiple {ALUs}.
The inputs to an {ALU} are the data to be operated on, called operands, and a code indicating the operation to be performed and, optionally, status information from a previous operation; the {ALU}'s output is the result of the performed operation. In many designs, the {ALU} also exchanges additional information with a status register, which relates to the result of the current or previous operations.},
	booktitle = {Wikipedia},
	urldate = {2017-03-16},
	date = {2017-03-13},
	langid = {english},
	note = {Page Version {ID}: 770066907},
	file = {Snapshot:/home/prodatalab/.zotero/zotero/7wd6pbl2.default/zotero/storage/GXM9U3XH/index.html:text/html}
}

@inreference{_service-oriented_2017,
	title = {Service-oriented architecture},
	rights = {Creative Commons Attribution-{ShareAlike} License},
	url = {https://en.wikipedia.org/w/index.php?title=Service-oriented_architecture&oldid=770220184},
	abstract = {A service-oriented architecture ({SOA}) is a style of software design where services are provided to the other components by application components, through a communication protocol over a network. The basic principles of service oriented architecture are independent of vendors, products and technologies. A service is a discrete unit of functionality that can be accessed remotely and acted upon and updated independently, such as retrieving a credit card statement online.
A service has four properties according to one of many definitions of {SOA}:
It logically represents a business activity with a specified outcome.
It is self-contained.
It is a black box for its consumers.
It may consist of other underlying services.
Different services can be used in conjunction to provide the functionality of a large software application. So far, the definition could be a definition of modular programming in the 1970s. Service-oriented architecture is less about how to modularize an application, and more about how to compose an application by integration of distributed, separately-maintained and deployed software components. It is enabled by technologies and standards that make it easier for components to communicate and cooperate over a network, especially an {IP} network.},
	booktitle = {Wikipedia},
	urldate = {2017-03-15},
	date = {2017-03-14},
	langid = {english},
	note = {Page Version {ID}: 770220184},
	file = {Snapshot:/home/prodatalab/.zotero/zotero/7wd6pbl2.default/zotero/storage/JNA26G3H/index.html:text/html}
}